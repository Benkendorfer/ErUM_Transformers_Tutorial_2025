{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this notebook, we will use the GPT-2 model to explore prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We can load the pretrained model from a repository called [Hugging Face](https://huggingface.co/openai-community/gpt2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Let's create a simple function to generate responses, given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_text(prompt, max_length=20, num_return_sequences=1):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate text\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and return generated sequences\n",
    "    return [tokenizer.decode(output_sequence, skip_special_tokens=True) for output_sequence in output_sequences]\n",
    "\n",
    "def print_response(prompt, n_responses=5, max_length=20):\n",
    "    generated_text = generate_text(\n",
    "        prompt, max_length=max_length,\n",
    "        num_return_sequences=n_responses\n",
    "    )\n",
    "\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    print(\"Generated Responses:\")\n",
    "    for response in generated_text:\n",
    "        print(f\"- {response}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "And we can test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response(\"Once upon a time in a land far away, there lived a\", n_responses=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "A very common problem with large language models is **hallucination** --- the confident reporting of false information as true. We can see that GPT-2, as a very simple model, is prone to hallucination. Let's try a few examples and see if we can beat the model and become better prompters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Capital of France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Q: What is is the capital of France?\\nA:\"\n",
    "\n",
    "print_response(query, n_responses=5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "You may notice that the responses are not very good at all. Maybe the model gets it right a couple of times, but likely it is inconsistent. Try to write a prompt that performs better, using prompt engineering techniques that you learned from the lecture. Make sure to change the `max_length` parameter as needed if your prompt grows longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Your query here\"\n",
    "print_response(query, n_responses=5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Simple math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Q: What is 5 x 3?\\nA:\"\n",
    "\n",
    "print_response(query, n_responses=5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This is remarkably bad. Again, see if you can come up with a prompt that can guide the model towards a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Your query here\"\n",
    "\n",
    "print_response(query, n_responses=5, max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "You may find that you can get the model to output a number, but perhaps not always the correct number. A simple LLM like GPT-2 does not have any built-in reasoning capabilities --- the only reasoning it can do is whatever is encoded in language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Large language models are trained on data produced by humans. This data comes with biases. It is worth noting that, unless we are careful, biases can make their way into a model through the training data, even if the model itself is not trained to be biased. In general, de-biasing models (both physics models and LLMs) is a difficult problem, and an active area of research.\n",
    "\n",
    "GPT-2 was trained on a large dataset without effective de-biasing. Let's look at a clear example of bias, to illustrate the problem. \n",
    "\n",
    "A word of warning: Note that the GPT produces random output, and might produce material that is violent, sexist, or racist. We have tried to give an example that is not too offensive in this regard; nevertheless, if you might be bothered, then we recommend that you skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response(\"The person is a man, so he works as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response(\"The person is a woman, so she works as\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Clearly the model has encoded some ideas about how gender determines a person's career. Other forms of bias can be more extreme, or more subtle. It is worth watching out for them.\n",
    "\n",
    "This is a problem not only socially, but also in your physics research. Training a model on a biased dataset (for example, a simulated dataset with mismodeling) can cause problems when the model is applied to measurements that it has not seen before. Be careful when using your models, and do not ever trust them completely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErUM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
