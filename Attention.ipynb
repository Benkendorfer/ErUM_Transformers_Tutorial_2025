{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We will use a dataset from <https://github.com/Charlie9/enron_intent_dataset_verified?tab=readme-ov-file>. This dataset consists of sentences from emails sent between employees of the Enron corporation. Each sentence has been manually labeled regarding whether it contains a request or does not contain a request. We will train an attention model to classify sentences as \"request\" or \"no request\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_intent_file(file_path: str) -> list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# Read positive and negative intent files\n",
    "pos_intent_path = \"data/Enron/intent_pos\"\n",
    "neg_intent_path = \"data/Enron/intent_neg\"\n",
    "\n",
    "pos_intent_sentences = read_intent_file(pos_intent_path)\n",
    "neg_intent_sentences = read_intent_file(neg_intent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We need to parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(\"Please send me the report by EOD.\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for txt in data_iter:\n",
    "        yield tokenizer(txt)\n",
    "\n",
    "all_sentences = pos_intent_sentences + neg_intent_sentences\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(all_sentences), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def encode_batch(batch):\n",
    "    # batch: list of (raw_sentence, label)\n",
    "    token_ids = [torch.tensor(vocab(tokenizer(txt)), dtype=torch.long)\n",
    "                 for txt, _ in batch]\n",
    "    padded = pad_sequence(token_ids, batch_first=True,\n",
    "                          padding_value=vocab['<pad>'])\n",
    "    labels = torch.tensor([lbl for _, lbl in batch], dtype=torch.long)\n",
    "    return padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import EnronRequestDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    token_ids_list, labels = zip(*batch)\n",
    "    # pad to batch max‚Äêlen\n",
    "    src = pad_sequence(token_ids_list,\n",
    "                       batch_first=True,\n",
    "                       padding_value=vocab['<pad>'])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    # pad_mask: True for PAD tokens, False for real tokens\n",
    "    pad_mask = src == vocab['<pad>']\n",
    "    return src, labels, pad_mask\n",
    "\n",
    "# ‚Äî now wrap in DataLoader ‚Äî\n",
    "sentences = pos_intent_sentences + neg_intent_sentences\n",
    "labels = [1] * len(pos_intent_sentences) + [0] * len(neg_intent_sentences)\n",
    "\n",
    "dataset = EnronRequestDataset(sentences, labels, vocab, tokenizer)\n",
    "loader  = DataLoader(dataset,\n",
    "                     batch_size=32,\n",
    "                     shuffle=True,\n",
    "                     collate_fn=collate_fn,\n",
    "                     num_workers=0,\n",
    "                     pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reload\n",
    "import importlib\n",
    "import attention\n",
    "importlib.reload(attention)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "model = attention.RequestClassifier(len(vocab))\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for src, labels, pad_mask in loader:\n",
    "        logits = model(src, src_key_padding_mask=pad_mask)\n",
    "        loss   = loss_fn(logits, labels)\n",
    "\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"üõë NaN in logits!\"); break\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step(); opt.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate single sentence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_sentence(model, sentence, vocab, tokenizer, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1) tokenize & numericalize\n",
    "        tokens = tokenizer(sentence)\n",
    "        ids    = torch.tensor(vocab(tokens), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        # 2) build padding mask (True==pad for Transformer)\n",
    "        pad_idx = vocab['<pad>']\n",
    "        mask    = ids != pad_idx\n",
    "        # 3) forward\n",
    "        logits = model(ids, src_key_padding_mask=~mask)\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "        pred   = probs.argmax(dim=-1).item()\n",
    "    return pred, probs.squeeze().cpu().tolist()\n",
    "\n",
    "# ‚Äî Example usage ‚Äî\n",
    "device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model     = model.to(device)            # your trained RequestClassifier\n",
    "\n",
    "test_sentences = [\n",
    "    \"Please send me the report by EOD.\",\n",
    "    \"I need the report ASAP.\",\n",
    "    \"Can you send me the report?\",\n",
    "    \"You used to have a white cat.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Knut is giving a lecture.\",\n",
    "    \"Knut, please give the lecture.\",\n",
    "    \"This is a test\",\n",
    "    \"Cats are blue. I want something blue. Get me a blue cat.\",\n",
    "    \"I am requesting a book.\",\n",
    "    \"I am requesting a book, please.\",\n",
    "    \"Please, I am requesting a book.\",\n",
    "    \"Please, please, I am requesting a book.\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    pred, probs = predict_sentence(model, sentence, vocab, tokenizer, device)\n",
    "\n",
    "    label_map = {0: \"no_request\", 1: \"request\"}\n",
    "    print(f\"‚Üí {sentence!r}\")\n",
    "    print(f\"Prediction: {label_map[pred]} (P(request)={probs[1]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utility\n",
    "importlib.reload(utility)\n",
    "\n",
    "def plot_attention(tokens, attn: np.ndarray):\n",
    "    # tokens: List[str], attn: [S,S] NumPy array\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(attn)             # one distinct plot, no seaborn\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=90)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    plt.xlabel(\"Key positions\"); plt.ylabel(\"Query positions\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "text = \"Please help me.\"\n",
    "tokens = tokenizer(text)\n",
    "ids    = torch.tensor([vocab(tokens)], dtype=torch.long)\n",
    "padm   = ids == vocab['<pad>']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, attn = model(ids, src_key_padding_mask=padm, return_attn=True)\n",
    "    # attn: [1, seq_len, seq_len]  (since single head)\n",
    "\n",
    "    attn_np = np.asarray(attn[0].detach().cpu().tolist())\n",
    "\n",
    "plot_attention(tokens, attn_np)\n",
    "\n",
    "col_importance = [ sum(row[j] for row in attn_np) for j in range(len(tokens)) ]\n",
    "\n",
    "# pair and sort\n",
    "token_scores = zip(tokens, col_importance)\n",
    "\n",
    "print(\"Top tokens by attention paid to them:\")\n",
    "for token, score in token_scores:\n",
    "    print(f\"  {token:>10} ‚Üí {score:.3f}\")\n",
    "\n",
    "utility.display_tokens_with_alpha(tokens, col_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Now let's plot a number of sentences, and see which words are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Please send me the report by EOD.\",\n",
    "    \"I need the report ASAP.\",\n",
    "    \"Please help me out with the report.\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    ids    = torch.tensor([vocab(tokens)], dtype=torch.long)\n",
    "    padm   = ids == vocab['<pad>']\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, attn = model(ids, src_key_padding_mask=padm, return_attn=True)\n",
    "        # attn: [1, seq_len, seq_len]  (since single head)\n",
    "\n",
    "        attn_np = np.asarray(attn[0].detach().cpu().tolist())\n",
    "\n",
    "    col_importance = [ sum(row[j] for row in attn_np) for j in range(len(tokens)) ]\n",
    "\n",
    "    utility.display_tokens_with_alpha(tokens, col_importance)\n",
    "    print(f\"Prediction: {label_map[logits.argmax(dim=-1).item()]} ; P(request)={F.softmax(logits, dim=-1)[0][1]:.4f}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErUM-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
