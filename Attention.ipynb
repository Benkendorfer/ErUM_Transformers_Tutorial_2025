{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4474a55f",
   "metadata": {},
   "source": [
    "# Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a4793",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7321e9a",
   "metadata": {},
   "source": [
    "We will use a dataset from <https://github.com/Charlie9/enron_intent_dataset_verified?tab=readme-ov-file>. This dataset consists of sentences from emails sent between employees of the Enron corporation. Each sentence has been manually labeled regarding whether it contains a request or does not contain a request. We will train an attention model to classify sentences as \"request\" or \"no request\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1fade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_intent_file(file_path: str) -> list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# Read positive and negative intent files\n",
    "pos_intent_path = \"data/Enron/intent_pos\"\n",
    "neg_intent_path = \"data/Enron/intent_neg\"\n",
    "\n",
    "pos_intent_sentences = read_intent_file(pos_intent_path)\n",
    "neg_intent_sentences = read_intent_file(neg_intent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb716c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7b21b",
   "metadata": {},
   "source": [
    "We need to parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a00dddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/b1/89gd0yf93jj8hsp0lkvmhbz00000gn/T/ipykernel_27725/3334903799.py\", line 1, in <module>\n",
      "    from torchtext.data.utils import get_tokenizer\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torchtext/__init__.py\", line 3, in <module>\n",
      "    from torch.hub import _get_torch_home\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/keesbenkendorfer/miniconda3/envs/ErUM-2/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1704987089515/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['please', 'send', 'me', 'the', 'report', 'by', 'eod', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(\"Please send me the report by EOD.\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc03ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for txt in data_iter:\n",
    "        yield tokenizer(txt)\n",
    "\n",
    "all_sentences = pos_intent_sentences + neg_intent_sentences\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(all_sentences), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def encode_batch(batch):\n",
    "    # batch: list of (raw_sentence, label)\n",
    "    token_ids = [torch.tensor(vocab(tokenizer(txt)), dtype=torch.long)\n",
    "                 for txt, _ in batch]\n",
    "    padded = pad_sequence(token_ids, batch_first=True,\n",
    "                          padding_value=vocab['<pad>'])\n",
    "    labels = torch.tensor([lbl for _, lbl in batch], dtype=torch.long)\n",
    "    return padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff0c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import EnronRequestDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    token_ids_list, labels = zip(*batch)\n",
    "    # pad to batch max‚Äêlen\n",
    "    src = pad_sequence(token_ids_list,\n",
    "                       batch_first=True,\n",
    "                       padding_value=vocab['<pad>'])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    # pad_mask: True for PAD tokens, False for real tokens\n",
    "    pad_mask = src == vocab['<pad>']\n",
    "    return src, labels, pad_mask\n",
    "\n",
    "# ‚Äî now wrap in DataLoader ‚Äî\n",
    "sentences = pos_intent_sentences + neg_intent_sentences\n",
    "labels = [1] * len(pos_intent_sentences) + [0] * len(neg_intent_sentences)\n",
    "\n",
    "dataset = EnronRequestDataset(sentences, labels, vocab, tokenizer)\n",
    "loader  = DataLoader(dataset,\n",
    "                     batch_size=32,\n",
    "                     shuffle=True,\n",
    "                     collate_fn=collate_fn,\n",
    "                     num_workers=0,\n",
    "                     pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b46a70ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3321\n",
      "Epoch 2/10, Loss: 0.8183\n",
      "Epoch 3/10, Loss: 0.1226\n",
      "Epoch 4/10, Loss: 0.4572\n",
      "Epoch 5/10, Loss: 0.1543\n",
      "Epoch 6/10, Loss: 0.5042\n",
      "Epoch 7/10, Loss: 0.0050\n",
      "Epoch 8/10, Loss: 0.1536\n",
      "Epoch 9/10, Loss: 0.0169\n",
      "Epoch 10/10, Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# import reload\n",
    "import importlib\n",
    "import attention\n",
    "importlib.reload(attention)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "model = attention.RequestClassifier(len(vocab))\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for src, labels, pad_mask in loader:\n",
    "        logits = model(src, src_key_padding_mask=pad_mask)\n",
    "        loss   = loss_fn(logits, labels)\n",
    "\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"üõë NaN in logits!\"); break\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step(); opt.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c693a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d232d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí 'Please send me the report by EOD.'\n",
      "Prediction: request (P(request)=1.0000)\n",
      "‚Üí 'I need the report ASAP.'\n",
      "Prediction: request (P(request)=0.8999)\n",
      "‚Üí 'Can you send me the report?'\n",
      "Prediction: request (P(request)=1.0000)\n",
      "‚Üí 'You used to have a white cat.'\n",
      "Prediction: no_request (P(request)=0.0000)\n",
      "‚Üí 'The weather is nice today.'\n",
      "Prediction: request (P(request)=0.7858)\n",
      "‚Üí 'Knut is giving a lecture.'\n",
      "Prediction: no_request (P(request)=0.0000)\n",
      "‚Üí 'Knut, please give the lecture.'\n",
      "Prediction: request (P(request)=0.8074)\n",
      "‚Üí 'This is a test'\n",
      "Prediction: no_request (P(request)=0.0000)\n",
      "‚Üí 'Cats are blue. I want something blue. Get me a blue cat.'\n",
      "Prediction: request (P(request)=0.9635)\n",
      "‚Üí 'I am requesting a book.'\n",
      "Prediction: no_request (P(request)=0.0000)\n",
      "‚Üí 'I am requesting a book, please.'\n",
      "Prediction: no_request (P(request)=0.1144)\n",
      "‚Üí 'Please, I am requesting a book.'\n",
      "Prediction: no_request (P(request)=0.1144)\n",
      "‚Üí 'Please, please, I am requesting a book.'\n",
      "Prediction: request (P(request)=0.6738)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate single sentence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_sentence(model, sentence, vocab, tokenizer, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1) tokenize & numericalize\n",
    "        tokens = tokenizer(sentence)\n",
    "        ids    = torch.tensor(vocab(tokens), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        # 2) build padding mask (True==pad for Transformer)\n",
    "        pad_idx = vocab['<pad>']\n",
    "        mask    = ids != pad_idx\n",
    "        # 3) forward\n",
    "        logits = model(ids, src_key_padding_mask=~mask)\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "        pred   = probs.argmax(dim=-1).item()\n",
    "    return pred, probs.squeeze().cpu().tolist()\n",
    "\n",
    "# ‚Äî Example usage ‚Äî\n",
    "device    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model     = model.to(device)            # your trained RequestClassifier\n",
    "\n",
    "test_sentences = [\n",
    "    \"Please send me the report by EOD.\",\n",
    "    \"I need the report ASAP.\",\n",
    "    \"Can you send me the report?\",\n",
    "    \"You used to have a white cat.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Knut is giving a lecture.\",\n",
    "    \"Knut, please give the lecture.\",\n",
    "    \"This is a test\",\n",
    "    \"Cats are blue. I want something blue. Get me a blue cat.\",\n",
    "    \"I am requesting a book.\",\n",
    "    \"I am requesting a book, please.\",\n",
    "    \"Please, I am requesting a book.\",\n",
    "    \"Please, please, I am requesting a book.\",\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    pred, probs = predict_sentence(model, sentence, vocab, tokenizer, device)\n",
    "\n",
    "    label_map = {0: \"no_request\", 1: \"request\"}\n",
    "    print(f\"‚Üí {sentence!r}\")\n",
    "    print(f\"Prediction: {label_map[pred]} (P(request)={probs[1]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90dc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErUM-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
