{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We will use a dataset from <https://github.com/Charlie9/enron_intent_dataset_verified?tab=readme-ov-file>. This dataset consists of sentences from emails sent between employees of the Enron corporation. Each sentence has been manually labeled regarding whether it contains a request or does not contain a request. We will train an attention model to classify sentences as \"request\" or \"no request\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_intent_file(file_path: str) -> list[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# Read positive and negative intent files\n",
    "pos_intent_path = \"data/Enron/intent_pos\"\n",
    "neg_intent_path = \"data/Enron/intent_neg\"\n",
    "\n",
    "pos_intent_sentences = read_intent_file(pos_intent_path)\n",
    "neg_intent_sentences = read_intent_file(neg_intent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Take a look at some of these sentences. Does the dataset look as you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    # print out some sentences and remove the ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now that we have the sentences, we need to parse them into tokens that can be fed to the model. Tokenization is a surprisingly complicated task which is highly language-dependent.\n",
    "\n",
    "(It is not as simple as identifying words; often parts of words are themselves individual tokens. The past-tense marker `-ed` must be separated from the verb `trained`, for example, to create two tokens: `train` and `ed`. German, then, requires a different algorithm --- the word `trainiert` clearly has the token `t` at the end, but then should the tokens be `trainieren` and `t`? Or `trainier` and `t`?).\n",
    "\n",
    "Luckily, we are physicists rather than linguists, and some very clever people have done the work already. We can parse the sentences using a pre-written tokenizer from PyTorch.\n",
    "\n",
    "NB: You may get an error like\n",
    "\n",
    "```txt\n",
    "A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.5 as it may crash.\n",
    "```\n",
    "\n",
    "You can safely ignore this warning and just re-run the cell. It should not affect the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(\"Please send me the report by EOD.\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Tokenize some sentences of your choosing and see what happens. Does it work as you expect, or are there any surprises? What if you try to tokenize a non-English sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize some sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now that we can tokenize individual sentences, we need to build up a vocabulary of tokens that appear in our training data. We will also add two new tokens to the vocabulary --- a token `<unk>` representing an unknown input, and a token `<pad>` for whitespace padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for txt in data_iter:\n",
    "        yield tokenizer(txt)\n",
    "\n",
    "all_sentences = pos_intent_sentences + neg_intent_sentences\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(all_sentences), specials=[\"<unk>\", \"<pad>\"])\n",
    "\n",
    "# We set the default token to be <unk>\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Take a look at a few entries of the `vocab` object. What sort of object is it? What does it map words onto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some vocab entries\n",
    "\n",
    "# print(vocab['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now let's train an attention-based model to classify sentences as requests or not. We will train an extremely simple model with a single attention head, just to show you how it all works.\n",
    "\n",
    "First, we need to load the data into a PyTorch `DataLoader` that can be passed to the model. This is necessary for easy parallelization of the training (though it is not critical for us in this application).\n",
    "\n",
    "We have hidden some technical details in the `data_management.py` file. Feel free to look at the implementation in there if you are curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_management import EnronRequestDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Gather sentences and assign labels\n",
    "sentences = pos_intent_sentences + neg_intent_sentences\n",
    "labels = [1] * len(pos_intent_sentences) + [0] * len(neg_intent_sentences)\n",
    "\n",
    "# Wrap the sentences in a DataLoader object\n",
    "dataset = EnronRequestDataset(sentences, labels, vocab, tokenizer)\n",
    "loader  = DataLoader(dataset,\n",
    "                     batch_size=32,\n",
    "                     shuffle=True,\n",
    "                     collate_fn=lambda batch: collate_fn(batch, vocab),\n",
    "                     num_workers=0,\n",
    "                     pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Now we can actually train the model! In order to best understand what is happening, we will use a very simple attention mechanism attached to a multilayer perceptron. The implementation has been hidden in the file `attention_model.py` --- feel free to look in there if you'd like! It's not as scary as you might think.\n",
    "\n",
    "For now, we will focus on trying to understand what the model is doing. First, we will load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import attention_model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "model = attention_model.RequestClassifier(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "What does the model look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Now let's see how the untrained model performs on the dataset. We can define a simple function to compute the true and false positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true positives, false positives, true negatives, and false negatives\n",
    "def compute_metrics(preds, labels):\n",
    "    tp = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    fp = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    tn = ((preds == 0) & (labels == 0)).sum().item()\n",
    "    fn = ((preds == 0) & (labels == 1)).sum().item()\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, labels, pad_mask in data_loader:\n",
    "            logits = model(src, src_key_padding_mask=pad_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    tp, fp, tn, fn = compute_metrics(all_preds, all_labels)\n",
    "    print(f\"True positive rate: {tp / (tp + fn) * 100:.2f}%\")\n",
    "    print(f\"False positive rate: {fp / (fp + tn) * 100:.2f}%\")\n",
    "    print(\"\")\n",
    "    print(f\"True negative rate: {tn / (tn + fp) * 100:.2f}%\")\n",
    "    print(f\"False negative rate: {fn / (fn + tp) * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "You should have found that the true and false positive (and true and false negative) rates were approximately 50%. This is expected for the untrained model --- it is just randomly guessing.\n",
    "\n",
    "Now let's perform the training. We can train the model over a small number of epochs using a simple binary cross-entropy loss function. Play around with the learning rate and number of epochs --- what do you find in terms of performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for src, labels, pad_mask in loader:\n",
    "        logits = model(src, src_key_padding_mask=pad_mask)\n",
    "        loss   = loss_fn(logits, labels)\n",
    "\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"🛑 NaN in logits!\"); break\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step(); opt.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We have trained a very simple model as a proof-of-concept, but think about what might be missing here, using what you have learned from the other tutorials in the workshop. Would you want to deploy this model in the real world? How could we make it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate its performance. First, we should compute the true positive and false positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "You probably found very good performance (unless something went wrong with the training). In fact, the true positive and true negative rates might be 100%. This is indicative of some possible overfitting in the model --- how could we avoid overfitting for future training?\n",
    "\n",
    "Now that we have the trained model, we can play around with it a bit. Let's write a function to evaluate a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate single sentence\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_sentence(model, sentence, vocab, tokenizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and map to vocabulary\n",
    "        tokens = tokenizer(sentence)\n",
    "        ids    = torch.tensor(vocab(tokens), dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # Build padding mask (necessary to pad empty characters)\n",
    "        pad_idx = vocab['<pad>']\n",
    "        mask    = ids != pad_idx\n",
    "\n",
    "        # Run the transformer\n",
    "        logits = model(ids, src_key_padding_mask=~mask)\n",
    "        probs  = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Return the class with the highest probability (0 or 1)\n",
    "        pred   = probs.argmax(dim=-1).item()\n",
    "\n",
    "    return pred, probs.squeeze().tolist()\n",
    "\n",
    "def print_prediction_for_sentence(sentence):\n",
    "    pred, probs = predict_sentence(model, sentence, vocab, tokenizer)\n",
    "\n",
    "    label_map = {0: \"no_request\", 1: \"request\"}\n",
    "    print(f\"→ {sentence!r}\")\n",
    "    print(f\"Prediction: {label_map[pred]} (P(request)={probs[1]:.4f})\")\n",
    "    print()\n",
    "\n",
    "test_sentences = [\n",
    "    \"Please send me the report today by the end of the day.\",\n",
    "    \"I need the report as soon as possible.\",\n",
    "    \"Can you send me the report?\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Knut is teaching a lecture on transformers.\",\n",
    "    \"Student, please evaluate the model performance.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print_prediction_for_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "This is looking pretty good! Try playing around now with your own sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a request that you might find in a business email (so it uses words that might be in the vocab)\n",
    "sentence = \"Write a request here.\"\n",
    "print_prediction_for_sentence(sentence)\n",
    "\n",
    "# Write a non-request that you might find in a business email\n",
    "sentence = \"Write a non-request here.\"\n",
    "print_prediction_for_sentence(sentence)\n",
    "\n",
    "# Write a request with some words that you think might not be in the vocab\n",
    "sentence = \"Write a request with non-vocab words here.\"\n",
    "print_prediction_for_sentence(sentence)\n",
    "\n",
    "# Write a request in another language (e.g., German)\n",
    "sentence = \"Schreiben Sie eine Anfrage in einer anderen Sprache hier.\"\n",
    "print_prediction_for_sentence(sentence)\n",
    "\n",
    "# Write any sentence of your choosing\n",
    "sentence = \"Your sentence here.\"\n",
    "print_prediction_for_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "What do you notice about the model behavior? Do certain words seem to cause the model to predict a sentence as being more request-like? Do certain requests consistently fail? Can you figure out how to modify an incorrectly-classified request in order to make it classify correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utility\n",
    "\n",
    "def plot_attention(tokens, attn: np.ndarray):\n",
    "    # tokens: List[str], attn: [S,S] NumPy array\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(attn)             # one distinct plot, no seaborn\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=90)\n",
    "    ax.set_yticklabels(tokens)\n",
    "    plt.xlabel(\"Key positions\"); plt.ylabel(\"Query positions\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "text = \"Please help me.\"\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "ids    = torch.tensor([vocab(tokens)], dtype=torch.long)\n",
    "padm   = ids == vocab['<pad>']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, attn = model(ids, src_key_padding_mask=padm, return_attn=True)\n",
    "    # attn: [1, seq_len, seq_len]  (since single head)\n",
    "\n",
    "    attn_np = np.asarray(attn[0].detach().cpu().tolist())\n",
    "\n",
    "plot_attention(tokens, attn_np)\n",
    "\n",
    "col_importance = [ sum(row[j] for row in attn_np) for j in range(len(tokens)) ]\n",
    "\n",
    "# pair and sort\n",
    "token_scores = zip(tokens, col_importance)\n",
    "\n",
    "print(\"Top tokens by attention paid to them:\")\n",
    "for token, score in token_scores:\n",
    "    print(f\"  {token:>10} → {score:.3f}\")\n",
    "\n",
    "utility.display_tokens_with_alpha(tokens, col_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Now let's plot a number of sentences, and see which words are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Please send me the report by EOD.\",\n",
    "    \"I need the report ASAP.\",\n",
    "    \"Please help me out with the report.\",\n",
    "    \"The boss would like the Facebook post today.\",\n",
    "    \"I hope all is well with you, and one day when you are in the DC area and have nothing better to do you will let me have the opportunity to visit over dinner and reflect on how many years it has been since you and we created the Natural Gas Clearing House.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokens = tokenizer(sentence)\n",
    "    ids    = torch.tensor([vocab(tokens)], dtype=torch.long)\n",
    "    padm   = ids == vocab['<pad>']\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, attn = model(ids, src_key_padding_mask=padm, return_attn=True)\n",
    "        # attn: [1, seq_len, seq_len]  (since single head)\n",
    "\n",
    "        attn_np = np.asarray(attn[0].detach().cpu().tolist())\n",
    "\n",
    "    col_importance = [ sum(row[j] for row in attn_np) for j in range(len(tokens)) ]\n",
    "\n",
    "    utility.display_tokens_with_alpha(tokens, col_importance)\n",
    "    print(f\"Prediction: {label_map[logits.argmax(dim=-1).item()]} ; P(request)={F.softmax(logits, dim=-1)[0][1]:.4f}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErUM-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
